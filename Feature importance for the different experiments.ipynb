{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve,precision_recall_curve, auc,confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import objectives\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_file_patient_label = cf.list_file_patient_label\n",
    "\n",
    "\n",
    "dict_patient_files = {\n",
    "    \"list_file_patient_data_5Bands\":cf.list_file_patient_data_5Bands,\n",
    "    \"list_file_patient_data_2hz\":cf.list_file_patient_data_2hz,\n",
    "    \"list_file_patient_data_EOG_features\":cf.list_file_patient_data_EOG_features,\n",
    "    \"list_file_patient_data_eeg_forehead_2hz\":cf.list_file_patient_data_eeg_forehead_2hz,\n",
    "    \"list_file_patient_data_eeg_forehead_5bands\":cf.list_file_patient_data_eeg_forehead_5bands}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_feature(file,eog):\n",
    "    data=scipy.io.loadmat(file)\n",
    "    transformations_performed = [element for element in data.keys() if not \"__\" in element]\n",
    "    all_feature_ = []\n",
    "    if not eog:\n",
    "        #print('here')\n",
    "        for transformation in transformations_performed:\n",
    "            feature_data = data[transformation]\n",
    "            #print(feature_data.shape,\"ff\")\n",
    "            channels,sample_number,frequency_bands = feature_data.shape\n",
    "            #print(feature_data.shape)!\n",
    "            feature_data=feature_data.reshape(sample_number,channels*frequency_bands)\n",
    "\n",
    "            all_feature_.append(feature_data)\n",
    "    else:\n",
    "        for transformation in transformations_performed:\n",
    "            #print(\"hi\")\n",
    "            feature_data = data[transformation]\n",
    "            all_feature_.append(feature_data)\n",
    "        \n",
    "    #print(np.array(all_feature_).shape)\n",
    "    patient_feature = np.concatenate(all_feature_,axis=-1)\n",
    "    #print(patient_feature.shape)\n",
    "    return patient_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(labels):\n",
    "    list_class=labels.copy()\n",
    "    for i,e in enumerate(list_class):\n",
    "        #print(i)\n",
    "        if e<0.35:\n",
    "            list_class[i]=0\n",
    "        elif e >=0.35 and e<0.70:\n",
    "            list_class[i]=1\n",
    "        else:\n",
    "            list_class[i]=2\n",
    "    return list_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(s):\n",
    "    return (s - s.min()) / (s.max() - s.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_pro_features(lst_patient,eog):\n",
    "    if eog:\n",
    "        all_patient_features = np.array([process_patient_feature(file,eog=eog) for file in lst_patient])\n",
    "    all_patient_features = np.array([process_patient_feature(file,eog=eog) for file in lst_patient])\n",
    "    return all_patient_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20355, 108)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "keys=dict_patient_files.keys()\n",
    "features_eog = [pre_pro_features(dict_patient_files[key],eog=True) for key in keys if \"EOG\" in key]\n",
    "features_eog = np.array(features_eog)[0]\n",
    "features_eog = features_eog.reshape(features_eog.shape[0]*features_eog.shape[1],features_eog.shape[2])\n",
    "features_eog.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(20355, 80)\n(20355, 188)\n"
     ]
    }
   ],
   "source": [
    "features_eeg = [pre_pro_features(dict_patient_files[key],eog=False) for key in keys if \"5bands\" in key]\n",
    "features_eeg = np.concatenate(features_eeg,axis=-1)\n",
    "features_eeg = features_eeg.reshape(features_eeg.shape[0]*features_eeg.shape[1],features_eeg.shape[2])\n",
    "print(features_eeg.shape)\n",
    "complete_dataset = np.concatenate([features_eeg,features_eog],axis=-1)\n",
    "print(complete_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "all_patient_labels = np.array([scipy.io.loadmat(file)['perclos'] for file in list_file_patient_label])\n",
    "number_patient,samples,_=all_patient_labels.shape\n",
    "labels = all_patient_labels.reshape(number_patient*samples)\n",
    "labels = (labels<threshold).astype(int)\n",
    "\n",
    "#labels=create_labels(labels)\n",
    "\n",
    "complete_dataset = min_max_scale(complete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-4049"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "np.count_nonzero(labels)-len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importances(complete_dataset, labels):\n",
    "    forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                                random_state=0)\n",
    "    forest.fit(complete_dataset, labels)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(complete_dataset.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    return indices,importances,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_encoder_ff = pd.DataFrame(columns=[\"f1 mean\",\"f1 std\",\"precision mean\",\"precision std\",\"recall mean\",\"recall std\"])\n",
    "def run_exp_for_feature_importance(name,complete_dataset,labels,z_mean=False,z_logsigma=False):\n",
    "\n",
    "    encoder_feature_importances =[]\n",
    "\n",
    "    for i in [8,16,32]:\n",
    "        encoder = keras.models.load_model('{}_{}.h5'.format(name,i))\n",
    "        if \"lstm\" in name:\n",
    "            new_dataset = encoder.predict(complete_dataset[:,:,np.newaxis])\n",
    "        else:\n",
    "            new_dataset = encoder.predict(complete_dataset)\n",
    "\n",
    "        if z_mean and \"ffvae\" in name:\n",
    "            new_dataset = np.array(new_dataset)[0,:,:]\n",
    "            indices,importances,std=feature_importances(new_dataset,labels)\n",
    "            encoder_feature_importances.append(pd.DataFrame({\"indices\":indices,\"features_importances\":importances,\"std\":std,}))\n",
    "        \n",
    "        elif z_logsigma and 'ffvae' in name:\n",
    "            new_dataset = np.array(new_dataset)[1,:,:]\n",
    "            indices,importances,std=feature_importances(new_dataset,labels)\n",
    "            encoder_feature_importances.append(pd.DataFrame({\"indices\":indices,\"features_importances\":importances,\"std\":std,}))\n",
    "\n",
    "        else:\n",
    "            indices,importances,std=feature_importances(new_dataset,labels)\n",
    "            encoder_feature_importances.append(pd.DataFrame({\"indices\":indices,\"features_importances\":importances,\"std\":std,}))\n",
    "    return encoder_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\youve\\anaconda3\\envs\\tf_old\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "C:\\Users\\youve\\anaconda3\\envs\\tf_old\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning:\n",
      "\n",
      "No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InternalError",
     "evalue": "2 root error(s) found.\n  (0) Internal: Blas GEMM launch failed : a.shape=(32, 188), b.shape=(188, 64), m=32, n=64, k=188\n\t [[{{node dense_12/MatMul}}]]\n\t [[dense_13/Relu/_27]]\n  (1) Internal: Blas GEMM launch failed : a.shape=(32, 188), b.shape=(188, 64), m=32, n=64, k=188\n\t [[{{node dense_12/MatMul}}]]\n0 successful operations.\n0 derived errors ignored.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-789475293f4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoder_feature_importance_ff\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mrun_exp_for_feature_importance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'encoder'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcomplete_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-f8449d6cbac1>\u001b[0m in \u001b[0;36mrun_exp_for_feature_importance\u001b[1;34m(name, complete_dataset, labels, z_mean, z_logsigma)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mnew_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mnew_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mz_mean\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"ffvae\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_old\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_old\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_old\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_old\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal: Blas GEMM launch failed : a.shape=(32, 188), b.shape=(188, 64), m=32, n=64, k=188\n\t [[{{node dense_12/MatMul}}]]\n\t [[dense_13/Relu/_27]]\n  (1) Internal: Blas GEMM launch failed : a.shape=(32, 188), b.shape=(188, 64), m=32, n=64, k=188\n\t [[{{node dense_12/MatMul}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    " encoder_feature_importance_ff =run_exp_for_feature_importance('encoder',complete_dataset,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_feature_importance_lstm = run_exp_for_feature_importance('encoder_lstm',complete_dataset,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_feature_importance_lstm_sigma=run_exp_for_feature_importance('encoder_lstm_z_logsigma',complete_dataset,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_feature_importance_ff_sigma= run_exp_for_feature_importance('encoder_ffvae',complete_dataset,labels,z_logsigma=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(encoder_features,encoder_features_vae,title,lstm=True):\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 15), sharey=True)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    df8,df16,df32=encoder_features \n",
    "    df8_,df16_,df32_=encoder_features_vae\n",
    "\n",
    "    # Bulbasaur\n",
    "    sns.barplot(ax=axes[0,0], x=df8.indices, y=df8.features_importances,yerr=df8[\"std\"][df8.indices])\n",
    "    if lstm:\n",
    "        axes[0,0].set_title(\"LSTM AE features\")\n",
    "    else:\n",
    "        axes[0,0].set_title(\"FF AE features\")\n",
    "\n",
    "\n",
    "\n",
    "    # Charmander\n",
    "    sns.barplot(ax=axes[0,1], x=df8_.indices, y=df8_.features_importances,yerr=df8_[\"std\"][df8_.indices])\n",
    "    if lstm:\n",
    "        axes[0,1].set_title(\"LSTM VAE features\")\n",
    "    else:\n",
    "        axes[0,1].set_title(\"FF VAE features\")\n",
    "\n",
    "\n",
    "    sns.barplot(ax=axes[1,0], x=df16.indices, y=df16.features_importances,yerr=df16[\"std\"][df16.indices])\n",
    "\n",
    "    # Charmander\n",
    "    sns.barplot(ax=axes[1,1], x=df16_.indices, y=df16_.features_importances,yerr=df16_[\"std\"][df16_.indices])\n",
    "\n",
    "\n",
    "    sns.barplot(ax=axes[2,0], x=df32.indices, y=df32.features_importances,yerr=df32[\"std\"][df32.indices])\n",
    "\n",
    "    # Charmander\n",
    "    sns.barplot(ax=axes[2,1], x=df32_.indices, y=df32_.features_importances,yerr=df32_[\"std\"][df32_.indices])\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar(encoder_feature_importance_ff,encoder_feature_importance_ff_sigma,title=\"VAE vs AE ff\",lstm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar(encoder_feature_importance_lstm,encoder_feature_importance_lstm_sigma,title=\"VAE vs AE LSTM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tf_old': conda)",
   "language": "python",
   "name": "python361064bittfoldcondad46f81c088cc4447b0401ce3155564e6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}